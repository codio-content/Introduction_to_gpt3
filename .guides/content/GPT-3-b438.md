##

**GPT-3**(Generative Pre-trained Transformer 3) launched in 2020 is the successor to GPT-2. GPT-3 is trained on over $175$ billion *parameters* on 45 TB of text from all over the internet. One of the datasets used for example is *Wikipedia*. The Wikipedia corpus has nearly 1 trillion words altogether.



GPT-3 is the third-generation language prediction model in the GPT-n series. This course will focus on a wide variety of tasks that we can perform with the GPT-3 model.

How **OpenAI's** GPT-3 works by giving an initial text as a prompt, then the program will produce text that continues the prompt.

For example, write a prompt on the text editor on the left, then click the **TRY IT** button below. An example prompt could be `how are you`.

{Try it!}(python3 box.py)

If you want to try other prompts, you can use the **Reset** button below to clear the text inside the file on the left. Then use the **TRY IT** button to generate another response. 

{Reset}(python3 reset.py)

The quality of the text generated by GPT-3 is very high. In other words, its ability to mimic human speech would make it hard to think of the response generated as simply a trained model. 

{Check It!|assessment}(multiple-choice-1302128436)
